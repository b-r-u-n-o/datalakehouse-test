{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95f31aa3-c1b3-4edf-80ed-d4d4407debd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting minio\n",
      "  Using cached minio-7.2.7-py3-none-any.whl (93 kB)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from minio) (21.3.0)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from minio) (1.26.13)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from minio) (4.4.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from minio) (2022.12.7)\n",
      "Collecting pycryptodome\n",
      "  Using cached pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->minio) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->minio) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio) (2.21)\n",
      "Installing collected packages: pycryptodome, minio\n",
      "Successfully installed minio-7.2.7 pycryptodome-3.20.0\n"
     ]
    }
   ],
   "source": [
    "! pip install minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6bb97b8-05b4-443b-a298-e1bde2f46815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "# import boto3\n",
    "# from botocore.client import Config\n",
    "from minio import Minio\n",
    "from confluent_kafka import Producer\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18b49e05-d28f-4ce8-921f-9e98206c386b",
   "metadata": {},
   "source": [
    "# ler dados API\n",
    "# configurar Kafka\n",
    "# enviar dados Kafka\n",
    "# job spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7610c7a6-501d-4918-8c3d-e1b98e5a751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = Minio(\"s3.amazonaws.com\")\n",
    "# client = Minio(\"s3.amazonaws.com\", \"datalake\", \"datalake\")\n",
    "client = Minio(\n",
    "    \"play.minio.io:9000\",\n",
    "    access_key=\"dHDgY1EjEVQkZkpa\",\n",
    "    secret_key=\"23x2hxwhlsWpy6TqH4nSpXG3IJ86vSgS\",\n",
    "    \n",
    ")\n",
    "# client = Minio(\"play.min.io\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bd7f715-e055-47f0-812b-03f868cf1c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Minio._list_objects at 0x7fb51d2b1d20>\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 's3a://raw/'\n",
    "\n",
    "# Para listar objetos no bucket 'raw'\n",
    "response = client.list_objects(bucket_name)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e4566-f0ac-4f40-80ba-29d627e1a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = client.list_buckets()\n",
    "for bucket in buckets:\n",
    "    print(bucket.name, bucket.creation_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "191d7f76-8f9a-4f25-aea7-31c4a0e05e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from confluent_kafka import Producer\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class ExtractDataApi:\n",
    "    \n",
    "    def __init__(self, url: str, request):\n",
    "        \n",
    "        self.req = request\n",
    "        self.url = url        \n",
    "    \n",
    "    def get_data(self):\n",
    "        \n",
    "        try:\n",
    "            response = self.req.get(self.url)\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "class KafkaProducer:\n",
    "    \n",
    "    def __init__(self, producer:Producer):\n",
    "        \n",
    "        self.producer = producer({'bootstrap.servers': 'kafka-broker:9092'})\n",
    "        \n",
    "    def send_data(self, topic:str, data:dict):\n",
    "        \n",
    "        try:\n",
    "            data = json.dumps(data)\n",
    "            self.producer.produce(topic, value=data.encode('utf-8'), callback=self.verify)\n",
    "            self.producer.flush()\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n",
    "    def verify(self, err, msg):\n",
    "        if err is not None:\n",
    "            logging.error(f'Message delivery failed: {err}')\n",
    "        else:\n",
    "            logging.info(f'Message delivered to {msg.topic} [{msg.partition()}], {msg.offset()}')\n",
    "\n",
    "class SparkStreaming:\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        # self.spark = SparkSession.builder \\\n",
    "        #     .appName('ContabilizeiTeste') \\\n",
    "        #     .getOrCreate()\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"MinIO Example\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9050\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", \"datalake\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", \"datalake\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", False) \\\n",
    "            .getOrCreate()\n",
    "    #     self.sc = self.spark.sparkContext\n",
    "    #     self._config_minio()\n",
    "    \n",
    "    # def _config_minio(self):\n",
    "    #     try:\n",
    "    #         self.sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"datalake\")\n",
    "    #         self.sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"datalake\")\n",
    "    #         self.sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://localhost:9050\")\n",
    "    #         self.sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "    #         self.sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    #         self.sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    #         self.sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "        \n",
    "        # except Exception as e:\n",
    "        #     raise e\n",
    "        \n",
    "    def read_data_streaming(self, topic:str):\n",
    "        \n",
    "        df = self.spark.readStream \\\n",
    "            .format('kafka') \\\n",
    "            .option('kafka.bootstrap.servers', 'kafka-broker:9092') \\\n",
    "            .option('subscribe', topic) \\\n",
    "            .load()\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def write_data_streaming(self, df, output_path:str):\n",
    "            \n",
    "        query = df.writeStream \\\n",
    "            .format('json') \\\n",
    "            .outputMode('append') \\\n",
    "            .option('path', output_path) \\\n",
    "            .option('checkpointLocation', 'checkpoint') \\\n",
    "            .trigger(processingTime='30 seconds') \\\n",
    "            .start()\n",
    "        \n",
    "        return query.awaitTermination()\n",
    "\n",
    "class DeltaOperations(SparkStreaming):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.spark = super().spark\n",
    "        self.delta_table = DeltaTable.forPath(self.spark, 'path')        \n",
    "\n",
    "    def merge_data(self, df):\n",
    "            \n",
    "        commit = (\n",
    "            self.delta_table\n",
    "                .alias('old') \n",
    "                .merge(\n",
    "                    df.alias('new'), \n",
    "                    'old.id = new.id'\n",
    "                ) \n",
    "                .whenMatchedUpdateAll(\n",
    "                  set={\n",
    "                    \"first_name\": \"new.first_name\",\n",
    "                    \"last_name\": \"new.last_name\",\n",
    "                    \"email\": \"new.email\",\n",
    "                    \"date_of_birth\": \"new.date_of_birth\"\n",
    "                }) \n",
    "                .whenNotMatchedInsertAll(\n",
    "                    values={\n",
    "                    \"id\": \"new.id\",\n",
    "                    \"first_name\": \"new.first_name\",\n",
    "                    \"last_name\": \"new.last_name\",\n",
    "                    \"email\": \"new.email\",\n",
    "                    \"date_of_birth\": \"new.date_of_birth\"\n",
    "                }\n",
    "            ) \n",
    "            .execute()\n",
    "        )\n",
    "        return commit            \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c281af4-8ebb-4698-8bb5-00b5ebf4fc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "\n",
    "    url = 'https://random-data-api.com/api/v2/users?size=1'\n",
    "    r = requests\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        print(\"Inicío da extração dos dados da API\")\n",
    "        e = ExtractDataApi(url, r)\n",
    "        time.sleep(1)\n",
    "        data = e.get_data()\n",
    "        \n",
    "        print(\"Iniciou a etapa Kafka\")\n",
    "        p = Producer\n",
    "        queue = KafkaProducer(p)\n",
    "        queue.send_data('user', data)\n",
    "        print(data)\n",
    "        \n",
    "        time.sleep(2)\n",
    "        print(\"Iniciou a etapa do Spark\")\n",
    "        spark = SparkStreaming()\n",
    "        topic = 'user'\n",
    "        df = spark.read_data_streaming(topic)\n",
    "        \n",
    "        print(\"Iniciou a gravação no S3\")\n",
    "        output_path = \"s3a://raw/users/\"\n",
    "        spark.write_data_streaming(df, output_path)\n",
    "        \n",
    "#         time.sleep(3)\n",
    "#         query = (\n",
    "#             df.writeStream \n",
    "#               .queryName(\"my_streaming_table\")  \n",
    "#               .outputMode(\"append\") \n",
    "#               .format(\"memory\") \n",
    "#               .start()\n",
    "#         )\n",
    "\n",
    "#         spark.sql(\"SELECT * FROM my_streaming_table\").show()\n",
    "\n",
    "#         query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238b5157-6d2b-4826-85ee-ca69f026a742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicío da extração dos dados da API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Message delivered to <built-in method topic of cimpl.Message object at 0x7fb50a583040> [0], 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciou a etapa Kafka\n",
      "{'id': 8934, 'uid': '7b9130d9-5a3f-48c4-9714-1d0917d10d3c', 'password': 'C36adYTGSw', 'first_name': 'Alva', 'last_name': 'Kautzer', 'username': 'alva.kautzer', 'email': 'alva.kautzer@email.com', 'avatar': 'https://robohash.org/quiaoccaecativoluptatem.png?size=300x300&set=set1', 'gender': 'Agender', 'phone_number': '+229 718-436-2636 x940', 'social_insurance_number': '778245308', 'date_of_birth': '1998-11-28', 'employment': {'title': 'Future Government Representative', 'key_skill': 'Organisation'}, 'address': {'city': 'West Hisako', 'street_name': 'Mraz Stream', 'street_address': '78153 Jennifer Ford', 'zip_code': '08859-1522', 'state': 'Connecticut', 'country': 'United States', 'coordinates': {'lat': 51.121701497848875, 'lng': 52.398825338796655}}, 'credit_card': {'cc_number': '4495101785945'}, 'subscription': {'plan': 'Bronze', 'status': 'Active', 'payment_method': 'Google Pay', 'term': 'Full subscription'}}\n",
      "Iniciou a etapa do Spark\n",
      "Iniciou a gravação no S3\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49c2d08-2111-4d68-9476-541ef26a91db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
